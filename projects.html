<!DOCTYPE html>

<html lang="en">
    <head>
      <meta charset="utf-8">

        <meta name="viewport" content="initial-scale=1, width=device-width">
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css">
        <link href="styles.css" rel="stylesheet">
        <!-- <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet"> -->
        <script src="https://code.jquery.com/jquery-1.10.2.js"></script> <!--Used for navigation bar-->
        <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.6.0/build/styles/agate.min.css">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <title>Projects</title>
    </head>
    <body>
      
        <!--Navigation bar-->
        <div id="nav-placeholder"> </div>
        <script>
            $(function(){
            $("#nav-placeholder").load("nav.html");
              });
        </script>
        <!--end of Navigation bar-->
        

<div class="container-fluid section-title mb-5">
    <!-- <h1> DATA SCIENCE PROJECTS</h1> -->
    <h1> Data Science Projects </h1>
</div>



<div class="container-fluid portfolio-container justify-content-center mx-md-auto mt-md-5">
    <div class="row justify-content-center mx-sm-3">

        <!-- first project here -->
        <div class="col justify-content-center center_phone mt-lg-5">
            <div class="container portfolio-hovering box mb-0">
                <a  data-bs-toggle="modal" data-bs-target="#project-1">
                    <img src="cat.JPG"  class="image resize-image">
                    <div class="portfolio-text-pos portfolio-text-hidden">
                        <div class="portfolio-text">
                            <p> Photos Categorizer
                            </p>
                            <hr>
                            <p> Transfer learning from MobileNet model to predict the category of a photo.
                            </p>
                        </div>
                    </div>
                    
                </a>
            </div>
        </div>


  </div>
</div>

<!-- The Modal -->
<div class="modal fade" id="project-1">
<div class="modal-dialog modal-dialog-centered modal-xl modal-dialog-scrollable">
  <div class="modal-content">

    <!-- Modal Header -->
    <div class="modal-header">
      <h3 class="modal-title w-100">Photos Classifier</h3>
      <button type="button" class="btn-close" data-bs-dismiss="modal"></button>
    </div>

    <!-- Modal body -->
    <div class="modal-body">
      A while ago I became interested in photography and bought a Fuji mirrorless camera.
      As time go on, the question of photos organization arises, especially thematical sorting.
      Furthermore, adding labels to my photos allow me to perform some statistics like determine the evolution of my interest in a subject (portrait, animal, ...). 
      Thus I create a CNN model to label my photos into one among 10 categories:
      <ul>
        <li> Animal</li>
        <li> Architecture
        <li> Astrophotography
        <li> Landscape
        <li> Macrophotography
        <li> Portrait
        <li> Sport
        <li> Still Life
        <li> Street Photography
        <li> Transport
      </ul>
      Since my laptop has no GPU and is not powerful (mid-range laptop 2018) I relied on transfer learning from MobileNetv2 detailed below.
      The repository is accessible on my <a href="https://github.com/quentgoutaland/RawPhotosClassifier/">Github</a>.
      
      <h4 text-align="center"> Dataset </h4>
        <p>The dataset is formed using the python flickr API from the module <code>flickrapi</code> along <code>urllib.request</code> to retrieve the photos url.
        The photos are retrieved using a search term, e.g., boat for transport and download in the folder corresponding to the class.</p>
        
        <p>Initially around 20000 photos were gathered like this.
        I then begin to clean the data. 
        First I removed misclassified photos manually.
        Then some photos seemed to be corrupted due to the presence of superfluous data, using the shell command
        <pre><code class="hljs  blockcode">mogrify -set comment "extraneous bytes removed" *.jpg</code></pre>
        allows to clean the extra useless data.
        The <code>mogrify</code> command from <code>ImageMagick</code> overwrite the existing photo applying some changes the user wants such that rotation or resizing.
        Since we just want to remove the extraneous bytes, we use the harmless <code> -set comment</code> transformation to all photos.
        Although I was able to keep most of corrupted photos with this command, some remain corrupted and therefore need to be removed from the dataset.
        Following <a href="https://stackoverflow.com/questions/62586443/tensorflow-error-when-trying-transfer-learning-invalid-jpeg-data-or-crop-windo">this</a> answer where first a minimal class <code>JPEG</code> is created (from this <a href="https://yasoob.me/posts/understanding-and-writing-jpeg-decoder-in-python/">post</a>):  
        <pre><code class="hljs blockcode">from struct import unpack

marker_mapping = {
    0xffd8: "Start of Image",
    0xffe0: "Application Default Header",
    0xffdb: "Quantization Table",
    0xffc0: "Start of Frame",
    0xffc4: "Define Huffman Table",
    0xffda: "Start of Scan",
    0xffd9: "End of Image"
}


class JPEG:
    def __init__(self, image_file):
        with open(image_file, 'rb') as f:
            self.img_data = f.read()
    
    def decode(self):
        data = self.img_data
        while(True):
            marker, = unpack(">H", data[0:2])
            # print(marker_mapping.get(marker))
            if marker == 0xffd8:
                data = data[2:]
            elif marker == 0xffd9:
                return
            elif marker == 0xffda:
                data = data[-2:]
            else:
                lenchunk, = unpack(">H", data[2:4])
                data = data[2+lenchunk:]            
            if len(data)==0:
                break        </code></pre>

  The <code>decode</code> function is then used to check the integrity of the image:
  <pre><code class="hljs blockcode">import os
from tqdm import tqdm

path = os.getcwd() + "/dataset/"
category_dir = [
              "animals/", 
              "architecture/",
              "astrophotography/", 
              "landscapes/", 
              "macrophotography/", 
              "portraits/",
              "sports/", 
              "still_lifes/", 
              "transports/", 
              "street_photos/"
            ]
          
images = [f"{path}{category_dir[i]}{filename}" 
          for i in range(len(category_dir)) 
          for filename in os.listdir(path + category_dir[i])
        ]
bads = []

for img in tqdm(images):
  image = JPEG(img) 
  try:
    image.decode()   
  except:
    bads.append(img)

for name in bads:
  os.remove(name)  </code></pre>

The first for loop gather the image names that can't be decode and the second one suppresses them.
</p> 
      
<p> After the cleaning, the dataset is consists of a total of 14737 photos whose distribution is given below</p>
<p>
  <img src="/figures/dataset_distribution.jpg" alt="dataset_distribution" class="img-portfolio">
</p>
        
      <h4 text-align="center"> Training </h4>
      <p>First, I select MobileNetv2 trained on imagenet as a base for my model for its efficiency on laptot without GPU.
        I then add three layers to get my model: a global average pooling layer to average every feature map;
        a dropout layer with a drop rate of 20% to avoid overfitting;
        and finally the softmax dense layer to select the most likely class of the photo.</p> 

      <p>
        <figure text-align="center">
          <img src="/figures/architecture_scheme.png" alt="architecture" class="img-portfolio">
          <figcaption> Model architecture</figcaption>

        </figure>
      </p>

      <p>
        The MobileNet weights are freezed, only the new layers parameters are trained.
        Xavier initialization is used to initialize the latter (<code>glorot_uniform</code>).
        Mini-batch gradient descent is used along Adam optimizer. 
        <pre><code class="hljs  blockcode">def photos_classifier_model(image_shape=(224, 224), classes=10):
  input_shape = image_shape + (3,)

  base_model = tf.keras.applications.MobileNetV2(
                input_shape = input_shape,
                include_top = False, 
                weights = "imagenet"
              )
  base_model.trainable = False 

  inputs = tf.keras.Input(shape = input_shape) 
  x = preprocess_input(inputs) 

  x = base_model(x, training = False) 
  x = tfl.GlobalAveragePooling2D()(x) 
  x = tfl.Dropout(rate = 0.2)(x)
  outputs = tfl.Dense(
              classes, 
              activation='softmax', 
              kernel_initializer = glorot_uniform()
            )(x)

  model = tf.keras.Model(inputs, outputs)

  return model</code></pre>

  <pre><code class="hljs  blockcode language-python">photo_model = photos_classifier_model()
base_learning_rate = 0.001
initial_epochs = 5
photo_model.compile(
  optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
  metrics=['accuracy']
)

photo_model.fit(
  train_dataset, 
  validation_data=validation_dataset, 
  epochs=initial_epochs
)
  </code></pre>

  With a learning rate of 0.001, a mini-batch size of 16 and 5 epochs, the accuracy of the dataset reach an accuracy of 96% on the training set and 93% on the validation set. 
  </p>
  <p>
    <figure text-align="center">
      <img src="/figures/accuracy_loss.jpg" alt="accuracy" class="img-portfolio-big">
      <!-- <figcaption> Model architecture</figcaption> -->

  </figure>
      </p>

      <h4 text-align="center"> Results </h4>
    <p> Although the model gives good result on many photos, especially portraits and transports, it has difficultiers on more complex subjects 
      (e.g., fishes often classed in macrophotography or choosing between architecture and street photography).
    </p>
    <p>
      Regarding the dataset, one can see selection biases for many classes. 
      For instance, searching photos with the keyword <q>landscape</q> gives mainly hills with blue sky photos with low variability.
      In the same way, submarine animals are missing in the animal category.
    </p>
    
    <p> 
      Therefore, adding more photos to the dataset will improve the accuracy of the model.
      As well as using data augmentation (transformations like rotation and flipping). 
    </p>
    <p>
      Finally, I plan to add another category in the future to account for social events.
    </p>
    <!-- Code Block example -->
    <!-- <pre><code class="hljs  blockcode"></code></pre> -->
    </div>

    </div>
</div>



    



      
      <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.6.0/build/highlight.min.js"></script>
      <!-- <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.6.0/build/languages/python.min.js"></script> -->
      <script>hljs.initHighlightingOnLoad();</script>

    </body>
</html>